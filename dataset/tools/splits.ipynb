{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-29T12:40:01.784703Z",
     "start_time": "2024-05-29T12:40:01.300150Z"
    }
   },
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "ROW_WIDTH = 108\n",
    "\n",
    "corpus_statistics = []\n",
    "corpus_statistics_answers = []\n",
    "\n",
    "def separated_filter(row):\n",
    "    return len(row.answers) > 0 and len(row.answers) != len(range(row.answers[0], row.answers[-1] + 1))\n",
    "\n",
    "\n",
    "def print_row(values, width=15):\n",
    "    print(f'{values[0]:^{35}} |', ' | '.join([f'{it:^{width}}' for it in values[1:]]))\n",
    "    \n",
    "\n",
    "def add_latex_table_row(values, table):\n",
    "    indentation = ''\n",
    "    table.append(f\"{indentation} {' & '.join([f'{it}' for it in values])} \\\\\\\\\")\n",
    "\n",
    "\n",
    "def print_split_stat(splits, get_value, name, digits=2, prefix=''):\n",
    "    values = [get_value(split) for split in splits]\n",
    "    rounded = [f'{value:.2f}' if digits > 0 else value for value in values]\n",
    "    print_row([name] + rounded)\n",
    "    add_latex_table_row([prefix, name] + rounded, table=corpus_statistics_answers)\n",
    "\n",
    "\n",
    "def print_percentage_stat(splits, get_column, name):\n",
    "    indentation = '\\hspace{1em}' if name in ['No Answer', 'Contiguous Answer', 'Non-Contiguous Answer'] else ''\n",
    "    values = [[get_column(split)[0], 100 * get_column(split)[0] / get_column(split)[1]] for split in splits]\n",
    "    rounded_print = [f'{value[0]} ({value[1]:.2f})' for value in values]\n",
    "    rounded_latex = [f'{value[0]} \\\\textcolor{{gray}}{{\\scriptsize ({value[1]:.0f}\\%)}}' for value in values]\n",
    "    print_row([name] + rounded_print)\n",
    "    add_latex_table_row(['', f'{indentation}{name}'] + rounded_latex, table=corpus_statistics_answers)\n",
    "\n",
    "\n",
    "def print_mean_std_stat(splits, get_column, name):\n",
    "    values = [[get_column(split).mean(), get_column(split).std()] for split in splits]\n",
    "    rounded_print = [f'{value[0]:.2f} ({value[1]:.2f})' for value in values]\n",
    "    rounded_latex = [f'{value[0]:.2f} \\\\textcolor{{gray}}{{\\scriptsize $\\pm$ {value[1]:.2f}}}' for value in values]\n",
    "    print_row([name] + rounded_print)\n",
    "    add_latex_table_row(['', name] + rounded_latex, table=corpus_statistics_answers)\n",
    "    \n",
    "\n",
    "def load_split(name, language):\n",
    "    if language is None:\n",
    "        return load_split(name, 'de')\n",
    "    split_path = f'../datasets/splits/{language}/{name}_{language}.json'\n",
    "    return pd.DataFrame(json.load(open(split_path, 'r')))\n",
    "\n",
    "\n",
    "def prepare_split(name, language):\n",
    "    split_df = load_split(name, language)\n",
    "    split_df['answer_count'] = split_df.apply(lambda x: len(x.answers), axis=1)\n",
    "    split_df['sentences'] = split_df.apply(lambda x: x.context.count('\\n') + 1, axis=1)\n",
    "    split_df['chars'] = split_df.apply(lambda x: len(x.context), axis=1)\n",
    "    split_df['question_word'] = split_df.apply(lambda x: x.question.split(' ')[0], axis=1)\n",
    "    split_df['question_chars'] = split_df.apply(lambda x: len(x.question), axis=1)\n",
    "    split_df['chars_per_sentence'] = split_df.apply(lambda x: x.chars / x.sentences, axis=1)\n",
    "    split_df['sentences_per_context'] = split_df.apply(lambda x: x.sentences, axis=1)\n",
    "    split_df['answers_per_sentence'] = split_df.apply(lambda x: x.answer_count / x.sentences, axis=1)\n",
    "    \n",
    "    contexts_df = split_df[split_df.duplicated(subset=['pageId']) != True]\n",
    "\n",
    "    if language is None:\n",
    "        return [split_df, contexts_df]\n",
    "    elif language == 'de':\n",
    "        de_split_df = split_df[(split_df.language == 'de') & (split_df.sourceLanguage != 'en')]\n",
    "        de_contexts_df = de_split_df[de_split_df.duplicated(subset=['pageId']) != True]\n",
    "        return [de_split_df, de_contexts_df]\n",
    "    else:\n",
    "        en_split_df = split_df[(split_df.language == 'en') & (split_df.sourceLanguage != 'de')]\n",
    "        en_contexts_df = en_split_df[en_split_df.duplicated(subset=['pageId']) != True]\n",
    "        return [en_split_df, en_contexts_df]\n",
    "\n",
    "\n",
    "def analyze_split_language_corpus(language):\n",
    "    train = prepare_split('train', language)\n",
    "    dev = prepare_split('dev', language)\n",
    "    test = prepare_split('test', language)\n",
    "    total = [pd.concat([train[i], dev[i], test[i]]) for i in range(2)]\n",
    "    splits = [train, dev, test, total]\n",
    "    \n",
    "    full_language = 'German' if language == 'de' else 'English' if language is not None else 'All' \n",
    "    \n",
    "    # Formatting\n",
    "    corpus_statistics.append('\\midrule')\n",
    "    corpus_statistics_answers.append('\\midrule')\n",
    "    # add_latex_table_row(['German' if language == 'de' else 'English' if language is not None else 'all', '', '', '', '', ''], table=corpus_statistics_answers)\n",
    "    \n",
    "    # Overall Stats\n",
    "    print_split_stat(splits, lambda dfs: len(dfs[0]), 'Questions', 0, prefix=full_language)\n",
    "    print_percentage_stat(splits, lambda dfs: [len(dfs[0][dfs[0].answers.str.len() == 0]), len(dfs[0])], 'No Answer')\n",
    "    print_percentage_stat(splits, lambda dfs: [len(dfs[0]) - len(dfs[0][dfs[0].answers.str.len() == 0]) - len(dfs[0][dfs[0].apply(separated_filter, axis=1)]), len(dfs[0])], 'Contiguous Answer')\n",
    "    print_percentage_stat(splits, lambda dfs: [len(dfs[0][dfs[0].apply(separated_filter, axis=1)]), len(dfs[0])], 'Non-Contiguous Answer')\n",
    "    \n",
    "    if language is not None:\n",
    "        # Corpus Stats\n",
    "        print('-' * ROW_WIDTH)\n",
    "        print_split_stat(splits, lambda dfs: len(dfs[1]), 'Documents', 0)\n",
    "        print_split_stat(splits, lambda dfs: len(dfs[0]) / len(dfs[1]), 'Questions/Document')\n",
    "        # TODO sum vs single?\n",
    "        # print_split_stat(splits, lambda dfs: dfs[1].chars.sum() / dfs[1].sentences.sum(), 'Chars/Sentence (sum)')\n",
    "        print_mean_std_stat(splits, lambda dfs: dfs[1].sentences, 'Sentences/Document')\n",
    "        print_mean_std_stat(splits, lambda dfs: dfs[1].chars_per_sentence, 'Chars/Sentence')\n",
    "        print_mean_std_stat(splits, lambda dfs: dfs[0].question_chars, 'Chars/Question')\n",
    "\n",
    "        # Answer Stats\n",
    "        print('-' * ROW_WIDTH)\n",
    "        # print_mean_std_stat(splits, lambda dfs: dfs[0].jaccard, 'Agreement (Jaccard)')\n",
    "        print_mean_std_stat(splits, lambda dfs: dfs[0].jaccard_cc_wo_adjacent, 'Agreement (Jaccard)')\n",
    "        print_mean_std_stat(splits, lambda dfs: dfs[0].jaccard_cc, 'with adjacent sentences')\n",
    "        # TODO w. vs w/o. answers?\n",
    "        print_mean_std_stat(splits, lambda dfs: dfs[0].answer_count, 'Answer Sentences/Question')\n",
    "        # print_mean_std_stat(splits, lambda dfs: dfs[0][dfs[0].answer_count > 0].answer_count, 'Answers/Question (w/o no answers)')\n",
    "        # TODO sum vs single?\n",
    "        # print_split_stat(splits, lambda dfs: dfs[0].answer_count.sum() / dfs[0].sentences.sum(), 'Answers/Sentence % (w no answers) (sum)')\n",
    "        # TODO w. vs w/o. answers?\n",
    "        print_mean_std_stat(splits, lambda dfs: dfs[0].answers_per_sentence, 'Answers Sentences/Total Sentences')\n",
    "        # print_mean_std_stat(splits, lambda dfs: dfs[0][dfs[0].answer_count > 0].answers_per_sentence, 'Answers/Sentence (w/o no answers)')\n",
    "\n",
    "\n",
    "def analyze_split_corpus():\n",
    "    print_row(['', 'train', 'dev', 'test', 'total'])\n",
    "    corpus_statistics_answers.append('\\\\toprule')\n",
    "    add_latex_table_row(['', '', 'train', 'dev', 'test', 'total'], corpus_statistics_answers)\n",
    "    \n",
    "    for language in ['de', 'en', None]:\n",
    "        print('#' * ROW_WIDTH)\n",
    "        # TODO english vs german w. english source\n",
    "        analyze_split_language_corpus(language)\n",
    "        \n",
    "    corpus_statistics_answers.append('\\\\bottomrule')\n",
    "\n",
    "    train = prepare_split('train', None)\n",
    "    dev = prepare_split('dev', None)\n",
    "    test = prepare_split('test', None)\n",
    "    total = [pd.concat([train[i], dev[i], test[i]]) for i in range(2)]\n",
    "    print()\n",
    "    print('#' * ROW_WIDTH)\n",
    "    print('Question words')\n",
    "    print('\\n'.join([f'{key}: {value}' for key, value in total[0].question_word.value_counts()[total[0].question_word.value_counts() > 5].items()]))\n",
    "    print(f'Other {total[0].question_word.value_counts()[total[0].question_word.value_counts() <= 5].sum()}')\n",
    "    \n",
    "    table_file = open('./resources/corpus.txt', 'w')\n",
    "    table_file.write('\\n'.join(corpus_statistics))\n",
    "\n",
    "    table_file = open('./resources/corpus_answers.txt', 'w')\n",
    "    table_file.write('\\n'.join(corpus_statistics_answers))\n",
    "\n",
    "\n",
    "analyze_split_corpus()"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T12:40:01.786321Z",
     "start_time": "2024-05-29T12:40:01.786201Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_df = load_split('train', 'en')\n",
    "train_df = train_df[train_df.jaccard == 1]\n",
    "ids = [65, 207, 13, 3241, 349]\n",
    "few_shot_examples_df = train_df[(train_df.id == ids[0]) | (train_df.id == ids[1]) | (train_df.id == ids[2]) | (train_df.id == ids[3]) | (train_df.id == ids[4])]\n",
    "\n",
    "window_size = 5\n",
    "\n",
    "few_shot_examples_df\n",
    "train_df"
   ],
   "id": "9413ad990ff070de",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
